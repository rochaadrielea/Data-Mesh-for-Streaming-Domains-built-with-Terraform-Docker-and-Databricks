# Databricks Full Pipeline (Bronze â†’ Silver â†’ Gold â†’ ML)

This repository showcases a complete end-to-end data engineering and machine learning pipeline built on Azure Databricks using PySpark. It is designed to ingest, clean, enrich, and train predictive models using real-time telemetry and operational data.

---

## ğŸ§± Architecture Overview

```
Terraform â†’ Azure Blob (Bronze) â†’ Databricks (Silver, Gold) â†’ ML Model â†’ MLflow â†’ Dashboard/Inference
```

---

## ğŸ¯ Use Case

This project simulates a **predictive maintenance** system for a rail or industrial fleet, where:
- Data from sensors and maintenance logs is streamed into a Bronze layer.
- Data is cleaned in Silver.
- Transformed into features in Gold.
- A machine learning model is trained and logged with MLflow.

---

## ğŸ§ª Technology Stack

| Layer        | Tools                                      |
|--------------|--------------------------------------------|
| Infrastructure | Terraform, Azure Resource Group/Blob Storage |
| Ingestion    | Databricks, PySpark, ABFSS                 |
| Processing   | Delta Lake (Bronze â†’ Silver â†’ Gold)        |
| ML & Logging | PySpark MLlib, MLflow                      |
| Secrets Mgmt | `.env`, `os.getenv()`                      |

---

## ğŸ” Secure Configuration

All secrets and sensitive values are stored as environment variables:

```bash
AZURE_STORAGE_ACCOUNT=your-storage-account-name
AZURE_CONTAINER_NAME=telemetry-data
AZURE_ACCESS_KEY=your-secret-access-key
```

Use the provided `.env.example` as a safe template.

---

## ğŸš¦ Pipeline Stages

### 1. Bronze Layer â€“ Real-Time Ingestion
- Streaming JSON files from simulated watchdog data
- Stored in Delta tables as raw historical data

### 2. Silver Layer â€“ Cleaning
- Drops fully-null rows
- Standardizes format for downstream processing

### 3. Gold Layer â€“ Enrichment
- Adds `date`, `time` columns
- Replaces all nulls with "unknown"
- Reorders columns for analytics/ML readiness

### 4. Machine Learning â€“ Model Training
- Filters telemetry_gold where `fault_code` exists
- Trains RandomForestClassifier on `engine_temp`, `speed`, `load`
- Logs accuracy and model with MLflow for versioning

---

## ğŸ“ Folder Structure

```
notebooks/
â””â”€â”€ databricks_pipeline/
    â””â”€â”€ full_pipeline_with_ml.py
.env.example
README.md
```

---

## ğŸ“Š Sample Output

```
âœ… Loaded 1050 rows into: telemetry_bronze
âœ… Processed Silver table: telemetry_silver
âœ… Saved Gold table: telemetry_gold
ğŸ¯ Accuracy: 0.88
âœ… Model logged with MLflow
```

---

## ğŸ“Œ MLflow Integration

The experiment is tracked in:
```
/Users/adriel.mlflow/telemetry_rf
```

It includes:
- Parameters (`features`, `label`)
- Accuracy metric
- Serialized Spark ML model

---

## ğŸ¤– Extend This Project

- Add SHAP for model interpretability
- Visualize fault detection results in Power BI
- Set up model scoring with Databricks Jobs or Airflow

---

## ğŸ‘©â€ğŸ’» Author

**Adriel Rocha**  
Azure Data Engineer | ML-Driven Architecture | Zurich ğŸ‡¨ğŸ‡­

---


---

## ğŸ” Model Interpretability with SHAP (Coming Soon)

To increase trust and transparency in the machine learning model, we plan to integrate **SHAP (SHapley Additive exPlanations)**:

- Visualize which features influence predictions most
- Explain anomalies and fault predictions
- Help domain experts understand model behavior

Example extension (coming soon):

```python
import shap

# Convert Spark DataFrame to Pandas
pandas_df = test_df.select("features").toPandas()

# Initialize SHAP explainer
explainer = shap.Explainer(model.featureImportances.toArray(), pandas_df)
shap_values = explainer(pandas_df)

# Plot
shap.summary_plot(shap_values, pandas_df)
```

This will be part of our next iteration of `full_pipeline_with_ml.py`.

---
